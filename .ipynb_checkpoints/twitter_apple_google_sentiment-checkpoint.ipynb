{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T22:57:12.504255Z",
     "start_time": "2020-06-02T22:57:08.403501Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T22:57:12.935008Z",
     "start_time": "2020-06-02T22:57:12.506089Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/michaelburak/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T22:57:12.941341Z",
     "start_time": "2020-06-02T22:57:12.936958Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/michaelburak/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T22:57:13.028648Z",
     "start_time": "2020-06-02T22:57:13.000927Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/michaelburak/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T22:57:14.849012Z",
     "start_time": "2020-06-02T22:57:14.813407Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>@teachntech00 New iPad Apps For #SpeechTherapy...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>#SXSW is just starting, #CTIA is around the co...</td>\n",
       "      <td>Android</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Beautifully smart and simple idea RT @madebyma...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Counting down the days to #sxsw plus strong Ca...</td>\n",
       "      <td>Apple</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>Excited to meet the @samsungmobileus at #sxsw ...</td>\n",
       "      <td>Android</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>Find &amp;amp; Start Impromptu Parties at #SXSW Wi...</td>\n",
       "      <td>Android App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>Foursquare ups the game, just in time for #SXS...</td>\n",
       "      <td>Android App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>Gotta love this #SXSW Google Calendar featurin...</td>\n",
       "      <td>Other Google product or service</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>Great #sxsw ipad app from @madebymany: http://...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>haha, awesomely rad iPad app by @madebymany ht...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>Holler Gram for iPad on the iTunes App Store -...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>I just noticed DST is coming this weekend. How...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>Just added my #SXSW flights to @planely. Match...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>Must have #SXSW app! RT @malbonster: Lovely re...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           tweet_text  \\\n",
       "0   .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1   @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2   @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3   @sxsw I hope this year's festival isn't as cra...   \n",
       "4   @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "5   @teachntech00 New iPad Apps For #SpeechTherapy...   \n",
       "6                                                 NaN   \n",
       "7   #SXSW is just starting, #CTIA is around the co...   \n",
       "8   Beautifully smart and simple idea RT @madebyma...   \n",
       "9   Counting down the days to #sxsw plus strong Ca...   \n",
       "10  Excited to meet the @samsungmobileus at #sxsw ...   \n",
       "11  Find &amp; Start Impromptu Parties at #SXSW Wi...   \n",
       "12  Foursquare ups the game, just in time for #SXS...   \n",
       "13  Gotta love this #SXSW Google Calendar featurin...   \n",
       "14  Great #sxsw ipad app from @madebymany: http://...   \n",
       "15  haha, awesomely rad iPad app by @madebymany ht...   \n",
       "16  Holler Gram for iPad on the iTunes App Store -...   \n",
       "17  I just noticed DST is coming this weekend. How...   \n",
       "18  Just added my #SXSW flights to @planely. Match...   \n",
       "19  Must have #SXSW app! RT @malbonster: Lovely re...   \n",
       "\n",
       "    emotion_in_tweet_is_directed_at  \\\n",
       "0                            iPhone   \n",
       "1                iPad or iPhone App   \n",
       "2                              iPad   \n",
       "3                iPad or iPhone App   \n",
       "4                            Google   \n",
       "5                               NaN   \n",
       "6                               NaN   \n",
       "7                           Android   \n",
       "8                iPad or iPhone App   \n",
       "9                             Apple   \n",
       "10                          Android   \n",
       "11                      Android App   \n",
       "12                      Android App   \n",
       "13  Other Google product or service   \n",
       "14               iPad or iPhone App   \n",
       "15               iPad or iPhone App   \n",
       "16                              NaN   \n",
       "17                           iPhone   \n",
       "18               iPad or iPhone App   \n",
       "19               iPad or iPhone App   \n",
       "\n",
       "   is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "0                                    Negative emotion  \n",
       "1                                    Positive emotion  \n",
       "2                                    Positive emotion  \n",
       "3                                    Negative emotion  \n",
       "4                                    Positive emotion  \n",
       "5                  No emotion toward brand or product  \n",
       "6                  No emotion toward brand or product  \n",
       "7                                    Positive emotion  \n",
       "8                                    Positive emotion  \n",
       "9                                    Positive emotion  \n",
       "10                                   Positive emotion  \n",
       "11                                   Positive emotion  \n",
       "12                                   Positive emotion  \n",
       "13                                   Positive emotion  \n",
       "14                                   Positive emotion  \n",
       "15                                   Positive emotion  \n",
       "16                 No emotion toward brand or product  \n",
       "17                                   Negative emotion  \n",
       "18                                   Positive emotion  \n",
       "19                                   Positive emotion  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('judge-1377884607_tweet_product_company.csv',encoding = \"ISO-8859-1\")\n",
    "\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T22:57:15.346473Z",
     "start_time": "2020-06-02T22:57:15.339675Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T22:57:16.546852Z",
     "start_time": "2020-06-02T22:57:16.541317Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_text                                            0\n",
       "emotion_in_tweet_is_directed_at                       0\n",
       "is_there_an_emotion_directed_at_a_brand_or_product    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renaming columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T22:57:17.436098Z",
     "start_time": "2020-06-02T22:57:17.423546Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9077</td>\n",
       "      <td>@mention your PR guy just convinced me to swit...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9079</td>\n",
       "      <td>&amp;quot;papyrus...sort of like the ipad&amp;quot; - ...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9080</td>\n",
       "      <td>Diller says Google TV &amp;quot;might be run over ...</td>\n",
       "      <td>Other Google product or service</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9085</td>\n",
       "      <td>I've always used Camera+ for my iPhone b/c it ...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9088</td>\n",
       "      <td>Ipad everywhere. #SXSW {link}</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3291 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0     .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1     @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2     @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3     @sxsw I hope this year's festival isn't as cra...   \n",
       "4     @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "...                                                 ...   \n",
       "9077  @mention your PR guy just convinced me to swit...   \n",
       "9079  &quot;papyrus...sort of like the ipad&quot; - ...   \n",
       "9080  Diller says Google TV &quot;might be run over ...   \n",
       "9085  I've always used Camera+ for my iPhone b/c it ...   \n",
       "9088                      Ipad everywhere. #SXSW {link}   \n",
       "\n",
       "      emotion_in_tweet_is_directed_at         sentiment  \n",
       "0                              iPhone  Negative emotion  \n",
       "1                  iPad or iPhone App  Positive emotion  \n",
       "2                                iPad  Positive emotion  \n",
       "3                  iPad or iPhone App  Negative emotion  \n",
       "4                              Google  Positive emotion  \n",
       "...                               ...               ...  \n",
       "9077                           iPhone  Positive emotion  \n",
       "9079                             iPad  Positive emotion  \n",
       "9080  Other Google product or service  Negative emotion  \n",
       "9085               iPad or iPhone App  Positive emotion  \n",
       "9088                             iPad  Positive emotion  \n",
       "\n",
       "[3291 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.rename(columns={\"tweet_text\": \"text\", \"emotion_in_text_is_directed_at\":\"direction\", \"is_there_an_emotion_directed_at_a_brand_or_product\":\"sentiment\"})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T22:57:18.644808Z",
     "start_time": "2020-06-02T22:57:18.642059Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = df['text'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T22:57:19.190428Z",
     "start_time": "2020-06-02T22:57:19.186478Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.@wesley83 I have a 3G iPhone. After 3 hrs tweeting at #RISE_Austin, it was dead!  I need to upgrade. Plugin stations at #SXSW.',\n",
       " \"@jessedee Know about @fludapp ? Awesome iPad/iPhone app that you'll likely appreciate for its design. Also, they're giving free Ts at #SXSW\",\n",
       " '@swonderlin Can not wait for #iPad 2 also. They should sale them down at #SXSW.',\n",
       " \"@sxsw I hope this year's festival isn't as crashy as this year's iPhone app. #sxsw\",\n",
       " \"@sxtxstate great stuff on Fri #SXSW: Marissa Mayer (Google), Tim O'Reilly (tech books/conferences) &amp; Matt Mullenweg (Wordpress)\",\n",
       " '#SXSW is just starting, #CTIA is around the corner and #googleio is only a hop skip and a jump from there, good time to be an #android fan',\n",
       " 'Beautifully smart and simple idea RT @madebymany @thenextweb wrote about our #hollergram iPad app for #sxsw! http://bit.ly/ieaVOB',\n",
       " 'Counting down the days to #sxsw plus strong Canadian dollar means stock up on Apple gear',\n",
       " 'Excited to meet the @samsungmobileus at #sxsw so I can show them my Sprint Galaxy S still running Android 2.1.   #fail',\n",
       " \"Find &amp; Start Impromptu Parties at #SXSW With @HurricaneParty http://bit.ly/gVLrIn I can't wait til the Android app comes out.\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T22:57:19.665286Z",
     "start_time": "2020-06-02T22:57:19.662897Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T22:57:20.755135Z",
     "start_time": "2020-06-02T22:57:20.751123Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.@wesley83 I have a 3G iPhone. After 3 hrs tweeting at #RISE_Austin, it was dead!  I need to upgrade'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "','.join(corpus)[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating stopwords list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T22:57:22.117422Z",
     "start_time": "2020-06-02T22:57:22.114854Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T22:57:22.674148Z",
     "start_time": "2020-06-02T22:57:22.666754Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_list = stopwords.words('english')\n",
    "stopwords_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T22:57:23.304207Z",
     "start_time": "2020-06-02T22:57:23.298469Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " '!',\n",
       " '\"',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " '[',\n",
       " '\\\\',\n",
       " ']',\n",
       " '^',\n",
       " '_',\n",
       " '`',\n",
       " '{',\n",
       " '|',\n",
       " '}',\n",
       " '~',\n",
       " '“',\n",
       " '”',\n",
       " '...',\n",
       " '``',\n",
       " \"''\",\n",
       " '’',\n",
       " '\\x89ûï']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "additional_punc = ['“','”','...','``',\"''\",'’','\\x89ûï']\n",
    "## Add punctuation to stopwords_list\n",
    "stopwords_list+=string.punctuation\n",
    "## Add additional_punc to stopwords_list\n",
    "stopwords_list.extend(additional_punc )\n",
    "stopwords_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T22:57:24.262041Z",
     "start_time": "2020-06-02T22:57:24.257197Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " '!',\n",
       " '\"',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '[',\n",
       " '\\\\',\n",
       " ']',\n",
       " '^',\n",
       " '_',\n",
       " '`',\n",
       " '{',\n",
       " '|',\n",
       " '}',\n",
       " '~',\n",
       " '“',\n",
       " '”',\n",
       " '...',\n",
       " '``',\n",
       " \"''\",\n",
       " '’',\n",
       " '\\x89ûï']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "stopwords_list = [e for e in stopwords_list if e not in ('@', '#')]\n",
    "stopwords_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T22:57:24.916760Z",
     "start_time": "2020-06-02T22:57:24.914126Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "tokenizer = nltk.tokenize.TweetTokenizer(preserve_case=False,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T22:57:30.034510Z",
     "start_time": "2020-06-02T22:57:28.635374Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foot\n",
      "running\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize('feet')) # foot\n",
    "print(lemmatizer.lemmatize('running')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenizer.tokenize,stop_words=stopwords_list)\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        text = lemmatizer.lemmatize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T22:36:30.918961Z",
     "start_time": "2020-06-02T22:36:30.913514Z"
    }
   },
   "outputs": [],
   "source": [
    "# def process_tweet(text,as_lemmas=False,as_tokens=True):\n",
    "# #     text=text.copy()\n",
    "#     for x in find_urls(text):\n",
    "#         text = text.replace(x,'')\n",
    "        \n",
    "#     for x in find_retweets(text):\n",
    "#         text = text.replace(x,'')    \n",
    "        \n",
    "#     for x in find_hashtags(text):\n",
    "#         text = text.replace(x,'')    \n",
    "\n",
    "#     if as_lemmas:\n",
    "#         from nltk.stem.wordnet import WordNetLemmatizer\n",
    "#         lemmatizer = WordNetLemmatizer()\n",
    "#         text = lemmatizer.lemmatize(text)\n",
    "    \n",
    "#     if as_tokens:\n",
    "#         text = clean_text(text)\n",
    "    \n",
    "#     if len(text)==0:\n",
    "#         text=''\n",
    "            \n",
    "#     return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T22:32:55.470887Z",
     "start_time": "2020-06-02T22:32:55.010835Z"
    }
   },
   "outputs": [],
   "source": [
    "# from nltk import word_tokenize\n",
    "# tokens = word_tokenize(','.join(corpus))\n",
    "# tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T22:32:55.751655Z",
     "start_time": "2020-06-02T22:32:55.562611Z"
    }
   },
   "outputs": [],
   "source": [
    "## Remove stopwords (create var stopped_tokens) and then re-product the FreqDist\n",
    "stopped_tokens = [word.lower() for word in tokens if word.lower() not in stopwords_list]\n",
    "stopped_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T22:32:55.795821Z",
     "start_time": "2020-06-02T22:32:55.753365Z"
    }
   },
   "outputs": [],
   "source": [
    "freq = FreqDist(stopped_tokens)\n",
    "freq.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T22:35:09.367322Z",
     "start_time": "2020-06-02T22:35:09.362469Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_text(text,regex=True):\n",
    "    from nltk.corpus import stopwords\n",
    "    import string\n",
    "    from nltk import word_tokenize,regexp_tokenize\n",
    "\n",
    "    ## tokenize text\n",
    "    if regex:\n",
    "        pattern = r\"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "        tokens= regexp_tokenize(text,pattern)\n",
    "    else:\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "    # Get all the stop words in the English language\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    stopwords_list += string.punctuation\n",
    "    \n",
    "    stopped_tokens = [w.lower() for w in tokens if w not in stopwords_list]\n",
    "    \n",
    "    return stopped_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T22:35:23.003659Z",
     "start_time": "2020-06-02T22:35:22.999799Z"
    }
   },
   "outputs": [],
   "source": [
    "## Other uses of RegEx for Tweet preprocessing\n",
    "import re\n",
    "\n",
    "def find_urls(string): \n",
    "    return re.findall(r\"(http[s]?://\\w*\\.\\w*/+\\w+)\",string)\n",
    "\n",
    "def find_hashtags(string):\n",
    "    return re.findall(r'\\#\\w*',string)\n",
    "\n",
    "def find_retweets(string):\n",
    "    return re.findall(r'RT [@]?\\w*:',string)\n",
    "\n",
    "def find_mentions(string):\n",
    "    return re.findall(r'\\@\\w*',string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T22:38:20.235223Z",
     "start_time": "2020-06-02T22:38:20.229405Z"
    }
   },
   "outputs": [],
   "source": [
    "y = df['sentiment'].copy()\n",
    "X = df['text'].copy()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T22:38:40.595010Z",
     "start_time": "2020-06-02T22:38:40.580845Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test, y_train,y_test = train_test_split(X,y,stratify=y,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T22:38:58.772353Z",
     "start_time": "2020-06-02T22:38:58.761191Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T22:39:58.604206Z",
     "start_time": "2020-06-02T22:39:58.301719Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T22:40:34.870818Z",
     "start_time": "2020-06-02T22:40:34.136519Z"
    }
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(class_weight='balanced')\n",
    "rf.fit(X_train_tfidf,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T22:41:12.385874Z",
     "start_time": "2020-06-02T22:41:12.264209Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "y_hat_train = rf.predict(X_train_tfidf)\n",
    "y_hat_test =  rf.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T22:41:13.292908Z",
     "start_time": "2020-06-02T22:41:12.956011Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_model(y_test,y_hat_test,X_test,clf=None,\n",
    "                  scoring=metrics.recall_score,verbose=False):\n",
    "\n",
    "    print(metrics.classification_report(y_test,y_hat_test))\n",
    "    metrics.plot_confusion_matrix(clf,X_test,y_test,normalize='true',\n",
    "                                 cmap='Blues')\n",
    "    plt.show()\n",
    "    if verbose:\n",
    "        print(\"MODEL PARAMETERS:\")\n",
    "        print(pd.Series(rf.get_params()))\n",
    "\n",
    "## Evaluate Model - Training Data\n",
    "evaluate_model(y_train, y_hat_train, X_train_tfidf, rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
